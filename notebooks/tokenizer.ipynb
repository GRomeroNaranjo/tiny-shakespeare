{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOU7a75zaPMQgoNjPwTwu5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"wgZ9oOibx3Kv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744844106862,"user_tz":-120,"elapsed":3857,"user":{"displayName":"guillem romero","userId":"01648526312521908562"}},"outputId":"08b70c42-2987-475f-e6b2-c50cfb8d8f6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!pip install tiktoken\n","from google.colab import drive\n","import sys\n","\n","drive.mount('/content/drive')\n","sys.path.append(\"/content/drive/MyDrive/Tiny-Shakespeare\")\n","\n","from minbpe.basic import BasicTokenizer\n","\n","tokenizer = BasicTokenizer()\n","with open(\"/content/drive/MyDrive/Tiny-Shakespeare/shakespeare-dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n","  text_sequence = f.read()"]},{"cell_type":"code","source":["tokenizer.train(text_sequence, vocab_size=1024)"],"metadata":{"id":"tnQXS_sw0DnL","executionInfo":{"status":"ok","timestamp":1744845071241,"user_tz":-120,"elapsed":649205,"user":{"displayName":"guillem romero","userId":"01648526312521908562"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["tokenizer.save(\"/content/drive/MyDrive/Tiny-Shakespeare/tokenizer_saved\")\n","tokenizer_2 = BasicTokenizer()\n","tokenizer_2.load(\"/content/drive/MyDrive/Tiny-Shakespeare/tokenizer_saved.model\")\n","\n","input_sequence = \"Where are thou romeo\"\n","encoded = tokenizer_2.encode(input_sequence)\n","decoded = tokenizer_2.decode(encoded)\n","input_sequence, encoded, decoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnUjclKU3va2","executionInfo":{"status":"ok","timestamp":1744845255041,"user_tz":-120,"elapsed":30,"user":{"displayName":"guillem romero","userId":"01648526312521908562"}},"outputId":"3608a9a2-364d-4aff-fc5e-b53289b475f5"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('Where are thou romeo',\n"," [375, 362, 420, 483, 452, 101, 111],\n"," 'Where are thou romeo')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def get_vocab_size(tokenizer):\n","  vocab_size = len(tokenizer.vocab)\n","  return vocab_size\n","\n","size_1 = get_vocab_size(tokenizer)\n","size_2 = get_vocab_size(tokenizer_2)\n","\n","size_1, size_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhFhY1514jJd","executionInfo":{"status":"ok","timestamp":1744845339671,"user_tz":-120,"elapsed":6,"user":{"displayName":"guillem romero","userId":"01648526312521908562"}},"outputId":"e0e8d12d-9511-4a5e-a6dc-9040de9bf15b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024, 1024)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.functional as F\n","\n","tokenized_dataset = tokenizer.encode(text_sequence)\n","tokenized_tensor = torch.tensor(tokenized_dataset, dtype=torch.long).unsqueeze(0)\n","tokenized_tensor[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjnX1k9V4xMV","executionInfo":{"status":"ok","timestamp":1744846253438,"user_tz":-120,"elapsed":824971,"user":{"displayName":"guillem romero","userId":"01648526312521908562"}},"outputId":"66676daf-9d23-44c2-ac02-b96db8085d72"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[332, 734,  49,  ...,  46, 738,  10]])"]},"metadata":{},"execution_count":11}]}]}
